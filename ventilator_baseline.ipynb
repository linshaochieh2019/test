{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ventilator_baseline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOw5Mj9GomkDlrIUv4jAJyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linshaochieh2019/test/blob/main/ventilator_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCce8Uz3VFHb",
        "outputId": "07086ab1-5839-4222-d517-d7d42bd2ed38"
      },
      "source": [
        "# Mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhX_KMb0Wuqw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz9NjCKyWi-7"
      },
      "source": [
        "df_csv = '/content/drive/MyDrive/Data/ventilator1/df_prep.csv'\n",
        "df = pd.read_csv(df_csv).iloc[:, 1:]\n",
        "\n",
        "rc_map = '/content/drive/MyDrive/Data/ventilator1/rcmap.csv'\n",
        "rc_map = pd.read_csv(rc_map).iloc[:, 1:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qAktKo3ieKK"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, input, rc_map, seq_len):\n",
        "    self.input = input\n",
        "    self.rc_map = rc_map\n",
        "    self.seq_len = seq_len\n",
        "    \n",
        "  def __getitem__(self, item):\n",
        "\n",
        "    # get breath_id\n",
        "    breath_id_temp = int(self.input[item][0].item())\n",
        "    rc = self.rc_map[self.rc_map['breath_id'] == breath_id_temp][['R','C']].values.squeeze()\n",
        "\n",
        "    data = {\n",
        "        'breath_indices': self.input[item:item + self.seq_len, 0],\n",
        "        'inputs': self.input[item:item + self.seq_len, 1:],\n",
        "        'targets': self.input[item + self.seq_len][1], #pressure\n",
        "        'rc': rc\n",
        "    }\n",
        "\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.input) - self.seq_len"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQFYFFyg9zjy"
      },
      "source": [
        "# scale pressure and u_in\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "ss_p = StandardScaler()\n",
        "ss_p.fit(np.array(df['pressure'])[:, np.newaxis])\n",
        "df['pressure'] = ss_p.transform(np.array(df['pressure'])[:, np.newaxis])\n",
        "\n",
        "ss_u_in = StandardScaler()\n",
        "ss_u_in.fit(np.array(df['u_in'])[:, np.newaxis])\n",
        "df['u_in'] = ss_u_in.transform(np.array(df['u_in'])[:, np.newaxis])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RERzPV0cijXD",
        "outputId": "b73069f0-59b0-452e-de78-59ed79aa438f"
      },
      "source": [
        "# We got too much data. Sampling just 10% for training\n",
        "\n",
        "input = df.iloc[:, :4].to_numpy() #breath_id, pressure, u_in, u_out\n",
        "input = torch.tensor(input, dtype=torch.float)\n",
        "\n",
        "dataset = MyDataset(input=input, rc_map=rc_map, seq_len=5)\n",
        "print(dataset[10])\n",
        "print(len(dataset))\n",
        "\n",
        "split_ratio = 0.9\n",
        "train_len = int(len(dataset) * split_ratio)\n",
        "val_len = len(dataset) - train_len\n",
        "\n",
        "_, train_set = random_split(dataset, \n",
        "                            lengths=[train_len, val_len], \n",
        "                            generator=torch.Generator().manual_seed(1))\n",
        "\n",
        "#train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "#val_loader = DataLoader(val_set, batch_size=16, shuffle=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'breath_indices': tensor([1., 1., 1., 1., 1.]), 'inputs': tensor([[0.4949, 1.5865, 0.0000],\n",
            "        [0.5035, 1.5980, 0.0000],\n",
            "        [0.6067, 1.5539, 0.0000],\n",
            "        [0.6970, 1.4944, 0.0000],\n",
            "        [0.6497, 1.5219, 0.0000]]), 'targets': tensor(0.6067), 'rc': array([1, 2])}\n",
            "7469545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VInvuDsSG4_h"
      },
      "source": [
        "# Only for trial\n",
        "#train_len = int(len(train_set) * split_ratio)\n",
        "#val_len = len(train_set) - train_len\n",
        "\n",
        "#train_set, val_set = random_split(train_set, \n",
        "#                                  lengths=[train_len, val_len], \n",
        "#                                  generator=torch.Generator().manual_seed(1))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WO5a10PMWF0"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, input_dim=3, dense_dim=128, lstm_dim=128, linear_dim=32): #input_dim: pressure, u_in, u_out\n",
        "    super().__init__()\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input_dim, dense_dim // 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(dense_dim // 2, dense_dim),\n",
        "        nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    self.lstm = nn.LSTM(dense_dim, lstm_dim, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.mlp(x)\n",
        "    features, (h,c) = self.lstm(features)\n",
        "    return h.squeeze()\n",
        "\n",
        "class DNNModel(nn.Module):\n",
        "  def __init__(self, input_dim=130): #feature shape 128 + [[r,c]] => 130 dims\n",
        "    super().__init__()\n",
        "    self.linear = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim//4, 1), #num_classes\n",
        "            )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    logits = self.linear(x)\n",
        "    return logits"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtWXGgO7xiBE",
        "outputId": "c4c45f3e-4780-4d4f-8134-7f6b09ce4525"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('GPU running: {}'.format(torch.cuda.get_device_name()))\n",
        "\n",
        "# Setup model and optimizer\n",
        "rnn = RNNModel()\n",
        "rnn.to(device)\n",
        "\n",
        "dnn = DNNModel()\n",
        "dnn.to(device)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU running: Tesla P100-PCIE-16GB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNNModel(\n",
              "  (linear): Sequential(\n",
              "    (0): Linear(in_features=130, out_features=32, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdqinNWYERP2"
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.HuberLoss()\n",
        "optimizer = optim.Adam(list(rnn.parameters()) + list(dnn.parameters()), lr=1e-3)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqbEtjP7Ejam",
        "outputId": "24b4e4a3-eecd-4080-e255-0e2c228e40c8"
      },
      "source": [
        "# training\n",
        "train_stats = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': []\n",
        "}\n",
        "\n",
        "best_val_loss = 1.\n",
        "\n",
        "for epoch in range(100):\n",
        "  print('Epoch #{}'.format(epoch))  \n",
        "  rnn.train()\n",
        "  dnn.train()\n",
        "  train_loss = 0.\n",
        "  \n",
        "  for batch in tqdm(train_loader):\n",
        "    breath_indices = batch['breath_indices']\n",
        "    inputs = batch['inputs'].to(device)\n",
        "    targets = batch['targets'].to(device)\n",
        "    rc = batch['rc'].to(device)\n",
        "    \n",
        "    # lstm\n",
        "    features = rnn(inputs)\n",
        "\n",
        "    # concat features and rc\n",
        "    input_cat = torch.cat((features, rc), dim=1)\n",
        "    logits = dnn(input_cat).squeeze()\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    loss = criterion(logits, targets)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  avg_train_loss = train_loss / len(train_loader)  \n",
        "  \n",
        "  # Validation phase\n",
        "  rnn.eval()\n",
        "  dnn.eval()\n",
        "  val_loss = 0.\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "      breath_indices = batch['breath_indices']\n",
        "      inputs = batch['inputs'].to(device)\n",
        "      targets = batch['targets'].to(device)\n",
        "      rc = batch['rc'].to(device)\n",
        "      \n",
        "      # lstm\n",
        "      features = rnn(inputs)\n",
        "\n",
        "      # concat features and rc\n",
        "      input_cat = torch.cat((features, rc), dim=1)\n",
        "      logits = dnn(input_cat).squeeze()\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion(logits, targets)\n",
        "      val_loss += loss.item()\n",
        "      \n",
        "    avg_val_loss = val_loss / len(val_loader)  \n",
        "\n",
        "    # save model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "      best_val_loss = avg_val_loss\n",
        "      print('Saving model weights ...')\n",
        "      weights_dir = '/content/drive/MyDrive/Colab Notebooks/ventilator/weights/'\n",
        "      exp_num = 'v1'\n",
        "      best_rnn = deepcopy(rnn.state_dict())\n",
        "      best_dnn = deepcopy(dnn.state_dict())\n",
        "      torch.save(best_rnn, weights_dir + exp_num + '_rnn.pt')\n",
        "      torch.save(best_dnn, weights_dir + exp_num + '_dnn.pt')\n",
        "\n",
        "    print('\\n')\n",
        "    print('train_loss: {:.6f}/ val_loss: {:.6f}'.format(avg_train_loss, avg_val_loss))\n",
        "    print('\\n')\n",
        "\n",
        "  # save training stats\n",
        "  train_stats['train_loss'].append(avg_train_loss)\n",
        "  train_stats['val_loss'].append(avg_val_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|â–Ž         | 691/21009 [00:29<14:16, 23.73it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oa5_arTGK0v"
      },
      "source": [
        "# plot the training outcome\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_stats['train_loss'])\n",
        "plt.plot(train_stats['val_loss'], color='red', linestyle='--')\n",
        "\n",
        "print('Best val_loss: {:.6f}'.format(best_val_loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}